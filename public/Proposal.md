#### We wish for an interface to life that has:

1. **Closure** : If a biochemical datum is digitized and non-proprietary - you should be able to get it. One API, one schema.

2. **Common Ontology** : Closure enables complete indexing and a singular namespace. This allows integration of wildly heterogenous data up to a `pd`-dataframe/`np`.shape/whatever is next.

**Poor data-integration** is something that plagues all fields of computational biology, i believe. It is subtle in its ubiquity: though both a geneticist and a bioelectricity researcher may be attending to the mechanisms of morphogenesis, (for aritrary reasons)  there is relatively little communication between their respective fields and hence fewer investemnet in the ontologies that bridge them. By the same token most software is designed for a very specific purpose and tends to get insulated in the space of its adjacet datatypes and instruments that bear it.

_A common ontology precludes a connected graph._ Biotechnology is ultimately an attempt to engineer a "better" outcome, if purely through prognostication at times. In that, the quality and accessibility of the tools comes to distinguish engineering from hacking in life-sciences of the future.

From a certain perspective, most \*nominally disconnected\* life-sciences fields are investigating the output of a single master-algorithm. Casting the data acquired by every field to a common biological assembly([ex.](https://pdb101.rcsb.org/learn/guide-to-understanding-pdb-data/biological-assemblies)) would provide a context common to each and permit richer inference. 
The practical argument in favor of this is roughly  _software_ and _data interoperability_, ease of communication. The less obvious and the more compelling one is the promise of _multi-scale inference_ that machine intelligence will deliver, shockingly still, in 2020s.
The realization that systems such as the output of that bespoke "master-algorithm" cannot be simultaneously apprehended and more importantly intuited over by a single human mind presently galvanizes the technology stack into assuming a new form [1](https://arxiv.org/abs/2003.08445),[2](https://arxiv.org/abs/2002.09571),[3](https://arxiv.org/pdf/1901.01753.pdf) that would accommodate a mind that does.

**Why? This would facilitate:**:

- _DNN Models that span verticals_ in addition to horizontals. [ Causality in biology](https://www.biorxiv.org/content/10.1101/2020.05.03.074419v1) is notoriously [ hard to establish ](https://doi.org/10.1155/2020/8932526)due to 2-way interactions between physical and structural scales, emergence. Models that learn from multiple scales/modalities of data have the potential to glean interactions that are not represented in the teleologically selected data. Overparametrization is a blessing that has to be capitalized upon.

- _Spanning graph databases_ [will elaborate].

- _Graph learning_ [*](https://arxiv.org/abs/1810.00826)


Given these observations, it seems reasonable to preoccupy myself with the infrastracutre (because none exists so far) for storing and chanelling knowledge to whatever the capable agent of the future looks like. There is no question in my mind about whether these lattices and bridges will be built, just the question of what is the best way to go about it.
