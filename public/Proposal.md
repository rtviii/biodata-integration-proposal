


#### We wish for an interface to life that has:

1. **Closure** : If a biochemical datum is digitized and non-proprietary - you should be able to get it. One API, one schema.  

2. **Common Ontology** : Closure enables complete indexing and a singular namespace. This allows integration of wildly heterogenous data up to a ```pd```-dataframe/```np```.shape/whatever is next.  


I believe that **poor data-integration is something that plagues all bio-scientific disciplines that accumulate any sizeable data**. It is also subtle in its ubiquity: though both the evolutionary geneticist and bioelectricity researcher may be attending to the mechanisms of morphogenesis or tumour formation, it is hardly the case that there is ample communication between their respective fields and hence fewer investemnet in the ontologies that bridge them. By the same token and to a larger extent, most software is designed for a very specific purpose and is insulated in the space of adjacet datatypes, lab-specific practices and instruments that bear it.   


*A common ontology precludes a connected graph.* Medicine, biotechnology, ecology, human assays into the fabric of life in general are ultimately attempts to engineer a "better" outcome, if purely through prognostication at times. Such attempts are motivated by intelligent co-observation of events which belong to different temporal and structural scales. The accessibility and reproducibility of these connections would come to distinguish engineering from hacking in life-sciences of the future. From a certain perspective, these nominally disconnected life-sciences fields are investigating the output of a single master-algorithm. Casting the data acquired by every field to a common [biological assembly](https://pdb101.rcsb.org/learn/guide-to-understanding-pdb-data/biological-assemblies) would provide a common context for each. The two obvious and pragmatic arguments to make in favor of this are roughly *ease of communication* and *software interoperability*. The less obvious and the most compelling one is the promise of much more lucid, *multi-scale inference* that machine intelligence will  deliver, shockingly still, in the 2020s. The realization that some systems cannot be simultaneously apprehended by a single human mind presently galvanizes the whole technology stack into assuming a new form that would accommodate a mind that does, albeit not explicitly human. 

There is no question about whether this infrastracture will be built, just the question of when to start building it and the evidence is abundant that the time was half a decade ago. 


**Why? This would facilitate:**:

- *DNN Models that span verticals* in addition to horizontals. Causality in biology is notoriously hard to establish due to 2-way interactions between physical and structural scales, emergence. Models that learn from multiple scales/modalities of data have the potential to glean interactions that are not represented in the teleologically selected data. Overparametrization is a blessing that has to be capitalized upon.
  
- *Spanning graph databases* [will elaborate].

- *Distributed mining*  [will elaborate].







